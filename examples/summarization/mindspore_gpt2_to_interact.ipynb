{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import logging\n",
    "from transformers import BertTokenizer\n",
    "from mindnlp.models.gpt2 import gpt2\n",
    "from mindspore import ops\n",
    "from mindspore.ops import operations as P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sets up the training arguments.\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--device', default='5', type=str, required=False, help='生成设备')\n",
    "parser.add_argument('--temperature', default=1, type=float, required=False, help='生成的temperature')\n",
    "parser.add_argument('--topk', default=10, type=int, required=False, help='最高k选1')\n",
    "parser.add_argument('--topp', default=0, type=float, required=False, help='最高积累概率')\n",
    "parser.add_argument('--model_config', default='/GPT2-Summary-mindspore/summary_model/config.json', type=str, required=False,\n",
    "                    help='模型参数')\n",
    "parser.add_argument('--log_path', default='GPT2-Summary-mindspore/data/interacting.log', type=str, required=False, help='interact日志存放位置')\n",
    "parser.add_argument('--voca_path', default='GPT2-Summary-mindspore/vocabulary/vocab_small.txt', type=str, required=False, help='选择词库')\n",
    "parser.add_argument('--dialogue_model_path', default='/home/daiyuxin/cjh1/news_summary/GPT2-Summary-mindspore/summary_model/model_epoch4', type=str, required=False, help='对话模型路径')\n",
    "parser.add_argument('--save_samples_path', default=\"GPT2-Summary-mindspore/sample/\", type=str, required=False, help=\"保存聊天记录的文件路径\")\n",
    "parser.add_argument('--repetition_penalty', default=1.2, type=float, required=False,\n",
    "                    help=\"重复惩罚参数，若生成的对话重复性较高，可适当提高该参数\")\n",
    "parser.add_argument('--seed', type=int, default=None, help='设置种子用于生成随机数，以使得训练的结果是确定的')\n",
    "parser.add_argument('--max_len', type=int, default=120, help='每个utterance的最大长度,超过指定长度则进行截断')\n",
    "parser.add_argument('--max_history_len', type=int, default=1, help=\"dialogue history的最大长度\")\n",
    "parser.add_argument('--no_cuda', default=False, help='不使用GPU进行预测')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger(args):\n",
    "    \"\"\"\n",
    "    将日志输出到日志文件和控制台\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # 创建一个handler，用于写入日志文件\n",
    "    file_handler = logging.FileHandler(\n",
    "        filename=args.log_path)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # 创建一个handler，用于将日志输出到控制台\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.DEBUG)\n",
    "    console.setFormatter(formatter)\n",
    "    logger.addHandler(console)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = create_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel<\n",
       "  (transformer): GPT2Model<\n",
       "    (wte): Embedding<vocab_size=13317, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=transformer.wte.embedding_table, shape=(13317, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "    (wpe): Embedding<vocab_size=1024, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=transformer.wpe.embedding_table, shape=(1024, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "    (drop): Dropout<>\n",
       "    (h): CellList<\n",
       "      (0): GPT2Block<\n",
       "        (ln_1): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.0.ln_1.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.0.ln_1.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPT2Attention<\n",
       "          (c_attn): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (attn_dropout): Dropout<>\n",
       "          (resid_dropout): Dropout<>\n",
       "          >\n",
       "        (ln_2): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.0.ln_2.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.0.ln_2.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (mlp): GPT2MLP<\n",
       "          (c_fc): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (act): GELU<>\n",
       "          (dropout): Dropout<>\n",
       "          >\n",
       "        >\n",
       "      (1): GPT2Block<\n",
       "        (ln_1): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.1.ln_1.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.1.ln_1.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPT2Attention<\n",
       "          (c_attn): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (attn_dropout): Dropout<>\n",
       "          (resid_dropout): Dropout<>\n",
       "          >\n",
       "        (ln_2): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.1.ln_2.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.1.ln_2.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (mlp): GPT2MLP<\n",
       "          (c_fc): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (act): GELU<>\n",
       "          (dropout): Dropout<>\n",
       "          >\n",
       "        >\n",
       "      (2): GPT2Block<\n",
       "        (ln_1): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.2.ln_1.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.2.ln_1.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPT2Attention<\n",
       "          (c_attn): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (attn_dropout): Dropout<>\n",
       "          (resid_dropout): Dropout<>\n",
       "          >\n",
       "        (ln_2): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.2.ln_2.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.2.ln_2.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (mlp): GPT2MLP<\n",
       "          (c_fc): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (act): GELU<>\n",
       "          (dropout): Dropout<>\n",
       "          >\n",
       "        >\n",
       "      (3): GPT2Block<\n",
       "        (ln_1): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.3.ln_1.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.3.ln_1.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPT2Attention<\n",
       "          (c_attn): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (attn_dropout): Dropout<>\n",
       "          (resid_dropout): Dropout<>\n",
       "          >\n",
       "        (ln_2): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.3.ln_2.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.3.ln_2.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (mlp): GPT2MLP<\n",
       "          (c_fc): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (act): GELU<>\n",
       "          (dropout): Dropout<>\n",
       "          >\n",
       "        >\n",
       "      (4): GPT2Block<\n",
       "        (ln_1): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.4.ln_1.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.4.ln_1.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPT2Attention<\n",
       "          (c_attn): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (attn_dropout): Dropout<>\n",
       "          (resid_dropout): Dropout<>\n",
       "          >\n",
       "        (ln_2): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.4.ln_2.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.4.ln_2.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (mlp): GPT2MLP<\n",
       "          (c_fc): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (act): GELU<>\n",
       "          (dropout): Dropout<>\n",
       "          >\n",
       "        >\n",
       "      (5): GPT2Block<\n",
       "        (ln_1): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.5.ln_1.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.5.ln_1.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPT2Attention<\n",
       "          (c_attn): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (attn_dropout): Dropout<>\n",
       "          (resid_dropout): Dropout<>\n",
       "          >\n",
       "        (ln_2): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.5.ln_2.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.5.ln_2.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (mlp): GPT2MLP<\n",
       "          (c_fc): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (act): GELU<>\n",
       "          (dropout): Dropout<>\n",
       "          >\n",
       "        >\n",
       "      (6): GPT2Block<\n",
       "        (ln_1): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.6.ln_1.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.6.ln_1.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPT2Attention<\n",
       "          (c_attn): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (attn_dropout): Dropout<>\n",
       "          (resid_dropout): Dropout<>\n",
       "          >\n",
       "        (ln_2): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.6.ln_2.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.6.ln_2.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (mlp): GPT2MLP<\n",
       "          (c_fc): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (act): GELU<>\n",
       "          (dropout): Dropout<>\n",
       "          >\n",
       "        >\n",
       "      (7): GPT2Block<\n",
       "        (ln_1): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.7.ln_1.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.7.ln_1.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPT2Attention<\n",
       "          (c_attn): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (attn_dropout): Dropout<>\n",
       "          (resid_dropout): Dropout<>\n",
       "          >\n",
       "        (ln_2): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.7.ln_2.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.7.ln_2.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (mlp): GPT2MLP<\n",
       "          (c_fc): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (act): GELU<>\n",
       "          (dropout): Dropout<>\n",
       "          >\n",
       "        >\n",
       "      (8): GPT2Block<\n",
       "        (ln_1): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.8.ln_1.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.8.ln_1.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPT2Attention<\n",
       "          (c_attn): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (attn_dropout): Dropout<>\n",
       "          (resid_dropout): Dropout<>\n",
       "          >\n",
       "        (ln_2): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.8.ln_2.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.8.ln_2.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (mlp): GPT2MLP<\n",
       "          (c_fc): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (act): GELU<>\n",
       "          (dropout): Dropout<>\n",
       "          >\n",
       "        >\n",
       "      (9): GPT2Block<\n",
       "        (ln_1): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.9.ln_1.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.9.ln_1.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (attn): GPT2Attention<\n",
       "          (c_attn): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (attn_dropout): Dropout<>\n",
       "          (resid_dropout): Dropout<>\n",
       "          >\n",
       "        (ln_2): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.h.9.ln_2.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.h.9.ln_2.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "        (mlp): GPT2MLP<\n",
       "          (c_fc): Conv1D<>\n",
       "          (c_proj): Conv1D<>\n",
       "          (act): GELU<>\n",
       "          (dropout): Dropout<>\n",
       "          >\n",
       "        >\n",
       "      >\n",
       "    (ln_f): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=transformer.ln_f.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=transformer.ln_f.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "    >\n",
       "  (lm_head): Dense<input_channels=768, output_channels=13317>\n",
       "  >"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型参数进入模型中\n",
    "tokenizer = BertTokenizer(vocab_file=args.voca_path)\n",
    "model = gpt2.GPT2LMHeadModel.from_pretrained(args.dialogue_model_path)\n",
    "model.set_train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. \n",
    "    \"\"\"\n",
    "    assert len(logits.shape) == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.shape[-1])  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        # mindspore.ops.operations.Topk()返回最后一维最大的top_k个元素，返回值为二维(values,indices)\n",
    "        # mindspore.ops.TopK()操作来获取一个Tensor中每一行的前k个最大值和它们的索引\n",
    "        # ...表示其他维度由计算机自行推断\n",
    "        topk = P.TopK(sorted=True)\n",
    "        less = ops.Less()\n",
    "        \n",
    "        indices_to_remove = less(logits, topk(logits, top_k)[0][-1])\n",
    "        logits[indices_to_remove] = filter_value  # 对于topk之外的其他元素的logits值设为负无穷\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = topk(logits, logits.shape[-1])  # 对logits进行递减排序\n",
    "        #cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        cumulative_probs = ops.cumsum(ops.softmax(sorted_logits), axis=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove_copy = mindspore.Tensor(sorted_indices_to_remove.asnumpy().copy())\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove_copy[..., :-1]\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************Summary model start************************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3548480/253630070.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# 最多生成max_len个token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_input_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cjh1/lib/python3.7/site-packages/mindspore/nn/cell.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0m_pynative_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m             \u001b[0m_pynative_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cjh1/lib/python3.7/site-packages/mindspore/nn/cell.py\u001b[0m in \u001b[0;36m_run_construct\u001b[0;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shard_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_forward_hook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cjh1/lib/python3.7/site-packages/mindnlp/models/gpt2/gpt2.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         )\n\u001b[1;32m    656\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cjh1/lib/python3.7/site-packages/mindspore/nn/cell.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0m_pynative_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m             \u001b[0m_pynative_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cjh1/lib/python3.7/site-packages/mindspore/nn/cell.py\u001b[0m in \u001b[0;36m_run_construct\u001b[0;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shard_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_forward_hook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cjh1/lib/python3.7/site-packages/mindnlp/models/gpt2/gpt2.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cjh1/lib/python3.7/site-packages/mindspore/common/_stub_tensor.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstub\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stub_shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstub_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstub_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('***********************Summary model start************************')\n",
    "\n",
    "with open(\"GPT2-Summary-mindspore/data/evaluation_with_ground_truth.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line.strip())\n",
    "        hypothesis = data[\"article\"]\n",
    "        for i in range(1):\n",
    "            if len(hypothesis) : hypothesis = hypothesis[:900]\n",
    "            input_ids = [tokenizer.cls_token_id]  # 每个input以[CLS]为开头\n",
    "            input_ids.extend(tokenizer.encode(hypothesis))\n",
    "            input_ids.append(tokenizer.sep_token_id)\n",
    "            curr_input_tensor = mindspore.Tensor(input_ids).astype(mindspore.int64)\n",
    "\n",
    "            generated = []\n",
    "            # 最多生成max_len个token\n",
    "            for _ in range(args.max_len):\n",
    "                outputs = model(input_ids=curr_input_tensor)\n",
    "                next_token_logits = outputs[0][-1, :]\n",
    "                \n",
    "                # 对于已生成的结果generated中的每个token添加一个重复惩罚项，降低其生成概率\n",
    "                for id in set(generated):\n",
    "                    next_token_logits[id] /= args.repetition_penalty\n",
    "                next_token_logits = next_token_logits / args.temperature\n",
    "                # 对于[UNK]的概率设为无穷小，也就是说模型的预测结果不可能是[UNK]这个token\n",
    "                next_token_logits[tokenizer.convert_tokens_to_ids('[UNK]')] = -float('Inf')\n",
    "                filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=args.topk, top_p=args.topp)\n",
    "                \n",
    "                # mindspore.ops.multinomial表示从候选集合中无放回地进行抽取num_samples个元素，权重越高，抽到的几率越高，返回元素的下标\n",
    "                next_token = ops.multinomial(ops.softmax(filtered_logits), 1, replacement=False).astype(mindspore.int64)\n",
    "                if next_token == tokenizer.sep_token_id:  # 遇到[SEP]则表明response生成结束\n",
    "                    break\n",
    "                generated.append(next_token.asnumpy().item())\n",
    "                concat = P.Concat(axis=0)\n",
    "                curr_input_tensor = concat(curr_input_tensor, next_token)\n",
    "                \n",
    "            hypothesis = tokenizer.convert_ids_to_tokens(generated)\n",
    "            reference = data[\"summarization\"]\n",
    "            hypothesis = ''.join(hypothesis)\n",
    "            with open('GPT2-Summary-mindspore/complete/hypothesis_mindspore.txt', 'a') as f1:\n",
    "                    f1.write(hypothesis.strip() + '\\n')\n",
    "            with open('GPT2-Summary-mindspore/complete/reference_mindspore.txt', 'a') as f2:\n",
    "                    f2.write(reference.strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "\n",
    "rouge = RougeCalculator(stopwords=True, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 读取生成的摘要文件和参考文件的内容\n",
    "with open('GPT2-Summary-mindspore/complete/hypothesis_mindspore.txt', 'r') as f:\n",
    "    generated_summary = f.read()\n",
    "with open('GPT2-Summary-mindspore/complete/reference_mindspore.txt', 'r') as f:\n",
    "    reference_summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.6419391573125102\n",
      "ROUGE-2: 0.3560618388934093\n",
      "ROUGE-L: 0.293151130632829\n"
     ]
    }
   ],
   "source": [
    " # 计算 ROUGE 分数\n",
    "rouge_1 = rouge.rouge_n(summary=generated_summary, references=[reference_summary], n=1)\n",
    "rouge_2 = rouge.rouge_n(summary=generated_summary, references=[reference_summary], n=2)\n",
    "rouge_l = rouge.rouge_l(summary=generated_summary, references=[reference_summary])\n",
    "\n",
    "# 打印 ROUGE 分数\n",
    "with open('GPT2-Summary-mindspore/complete/score_mindspore.txt', 'w') as f:\n",
    "    f.write(\"对得分进行平均之后的结果(mindspore): \"+ '\\n')\n",
    "    f.write(\"ROUGE-1的输出为: \" + str(rouge_1) + '\\n')\n",
    "    f.write(\"ROUGE-2的输出为: \" + str(rouge_2) + '\\n')\n",
    "    f.write(\"ROUGE-L的输出为: \" + str(rouge_l) + '\\n')\n",
    "print('ROUGE-1:', rouge_1)\n",
    "print('ROUGE-2:', rouge_2)\n",
    "print('ROUGE-L:', rouge_l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
