{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] ME(44611:140086138365760,MainProcess):2023-05-10-14:56:57.339.080 [mindspore/run_check/_check_version.py:226] Cuda ['10.1', '11.1', '11.6'] version(libcu*.so need by mindspore-gpu) is not found. Please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, or check whether the CUDA version in wheel package and the CUDA runtime in current device matches. Please refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[ERROR] ME(44611:140086138365760,MainProcess):2023-05-10-14:56:57.363.851 [mindspore/run_check/_check_version.py:226] Cuda ['10.1', '11.1', '11.6'] version(libcudnn*.so need by mindspore-gpu) is not found. Please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, or check whether the CUDA version in wheel package and the CUDA runtime in current device matches. Please refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "/data/miniconda3/envs/mindspore/lib/python3.7/site-packages/mindnlp/utils/download.py:26: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "import argparse\n",
    "import numpy as np\n",
    "import logging\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from mindspore.nn import CrossEntropyLoss\n",
    "from mindspore import nn, ops\n",
    "from mindspore.train.serialization import save_checkpoint\n",
    "from mindspore.dataset import TextFileDataset\n",
    "\n",
    "from mindnlp.transforms import BertTokenizer\n",
    "from mindnlp.modules import Accumulator\n",
    "from mindnlp.models import GPT2Config, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.utils import cache_file\n",
    "\n",
    "url = 'https://download.mindspore.cn/toolkits/mindnlp/dataset/text_generation/nlpcc2017/train_with_summ.txt'\n",
    "path, _ = cache_file('train_with_summ.txt', './', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TextFileDataset(str(path), shuffle=False)\n",
    "dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset, test_dataset = dataset.split([0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# article: [CLS] xxxxx [SEP]\n",
    "# summary: [CLS] xxxxx [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_dataset(dataset, tokenizer, batch_size=32, max_seq_len=1024, shuffle=False):\n",
    "    def read_map(text):\n",
    "        data = json.loads(text.tobytes())\n",
    "        return np.array(data['article']), np.array(data['summarization'])\n",
    "\n",
    "    def merge_and_pad(article, summary):\n",
    "        article_len = len(article)\n",
    "        summary_len = len(summary)\n",
    "\n",
    "        sep_id = np.array([tokenizer.sep_token_id])\n",
    "        pad_id = np.array([tokenizer.pad_token_id])\n",
    "        if article_len + summary_len > max_seq_len:\n",
    "            new_article_len = max_seq_len - summary_len\n",
    "            merged = np.concatenate([article[:new_article_len], sep_id, summary[1:]])\n",
    "        elif article_len + summary_len - 1 < max_seq_len:\n",
    "            pad_len = max_seq_len - article_len - summary_len + 1\n",
    "            pad_text = np.array([tokenizer.pad_token_id] * pad_len)\n",
    "            merged = np.concatenate([article, summary[1:], pad_text])\n",
    "        else:\n",
    "            merged = np.concatenate([article, summary[1:]])\n",
    "        \n",
    "        labels = np.concatenate([merged[1:], pad_id])\n",
    "            \n",
    "        return merged, labels\n",
    "\n",
    "    dataset = dataset.map(read_map, 'text', ['article', 'summary'])\n",
    "    dataset = dataset.map(tokenizer, 'article')\n",
    "    dataset = dataset.map(tokenizer, 'summary')\n",
    "    dataset = dataset.map(merge_and_pad, ['article', 'summary'], ['input_ids', 'labels'])\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = process_dataset(train_dataset, tokenizer)\n",
    "eval_dataset = process_dataset(eval_dataset, tokenizer)\n",
    "test_dataset = process_dataset(test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(shape=[32, 1024], dtype=Int64, value=\n",
       " [[ 101, 8119,  118 ...    0,    0,    0],\n",
       "  [ 101, 2190, 3683 ...    0,    0,    0],\n",
       "  [ 101, 3862, 1366 ...    0,    0,    0],\n",
       "  ...\n",
       "  [ 101, 1355, 2357 ...    0,    0,    0],\n",
       "  [ 101, 1298, 3736 ... 7564, 6356,  102],\n",
       "  [ 101, 5468, 5143 ... 4895, 2042,  102]]),\n",
       " Tensor(shape=[32, 1024], dtype=Int64, value=\n",
       " [[8119,  118, 8140 ...    0,    0,    0],\n",
       "  [2190, 3683,  671 ...    0,    0,    0],\n",
       "  [3862, 1366, 1159 ...    0,    0,    0],\n",
       "  ...\n",
       "  [1355, 2357, 3189 ...    0,    0,    0],\n",
       "  [1298, 3736, 1344 ... 6356,  102,    0],\n",
       "  [5468, 5143,  100 ... 2042,  102,    0]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataset.create_tuple_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size = 8\n",
    "\n",
    "lr = 1e-4\n",
    "warmup_steps = 2000\n",
    "accumulate_step = 2\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "log_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = GPT2Config(vocab_size=len(tokenizer))\n",
    "model = GPT2LMHeadModel(config, ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = nn.AdamWeightDecay(model.trainable_params(), lr)\n",
    "accumulator = Accumulator(optimizer, accumulate_step, max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define forward function\n",
    "def forward_fn(input_ids, labels):\n",
    "    logits = model(input_ids, labels=labels)\n",
    "    return loss / accumulate_step\n",
    "\n",
    "# Get gradient function\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, model.trainable_params())\n",
    "\n",
    "# Define function of one-step training\n",
    "# @mindspore.jit\n",
    "def train_step(data, label):\n",
    "    loss, grads = grad_fn(data, label)\n",
    "    accumulator(grads)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of model parameters: 118295040\n"
     ]
    }
   ],
   "source": [
    "# 记录模型参数数量\n",
    "num_parameters = 0\n",
    "parameters = model.trainable_params()\n",
    "for parameter in parameters:\n",
    "    num_parameters += parameter.numel()\n",
    "print('number of model parameters: {}'.format(num_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch_idx, (input_ids, labels) in enumerate(train_dataset.create_tuple_iterator()):\n",
    "        loss = train_step(input_ids, labels)\n",
    "\n",
    "        if batch_idx % log_step == 0:\n",
    "            loss, current = loss.asnumpy(), batch_idx\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
