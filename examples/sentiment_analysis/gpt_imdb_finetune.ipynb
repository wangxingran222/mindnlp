{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] ME(26283:140321715054400,MainProcess):2023-05-07-01:30:21.933.774 [mindspore/run_check/_check_version.py:226] Cuda ['10.1', '11.1', '11.6'] version(libcu*.so need by mindspore-gpu) is not found. Please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, or check whether the CUDA version in wheel package and the CUDA runtime in current device matches. Please refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[ERROR] ME(26283:140321715054400,MainProcess):2023-05-07-01:30:21.954.354 [mindspore/run_check/_check_version.py:226] Cuda ['10.1', '11.1', '11.6'] version(libcudnn*.so need by mindspore-gpu) is not found. Please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, or check whether the CUDA version in wheel package and the CUDA runtime in current device matches. Please refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "/data/miniconda3/envs/mindspore/lib/python3.7/site-packages/mindnlp/utils/download.py:26: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[WARNING] ME(26283:140321715054400,MainProcess):2023-05-07-01:30:23.206.571 [mindspore/ops/operations/math_ops.py:4741] The 'NPUAllocFloatStatus' operator will be deprecated in the future. Please don't use it.\n",
      "[WARNING] ME(26283:140321715054400,MainProcess):2023-05-07-01:30:23.207.468 [mindspore/ops/operations/math_ops.py:4875] The 'NPUClearFloatStatus' operator will be deprecated in the future. Please don't use it.\n",
      "[WARNING] ME(26283:140321715054400,MainProcess):2023-05-07-01:30:23.207.997 [mindspore/ops/operations/math_ops.py:4811] The 'NPUGetFloatStatus' operator will be deprecated in the future. Please don't use it.\n",
      "[WARNING] ME(26283:140321715054400,MainProcess):2023-05-07-01:30:23.208.520 [mindspore/common/api.py:844] 'mindspore.ms_class' will be deprecated and removed in a future version. Please use 'mindspore.jit_class' instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import mindspore\n",
    "from mindspore.dataset import text, GeneratorDataset, transforms\n",
    "from mindspore import nn\n",
    "\n",
    "from mindnlp import load_dataset\n",
    "from mindnlp.transforms import PadTransform, GPTTokenizer\n",
    "\n",
    "from mindnlp.engine import Trainer, Evaluator\n",
    "from mindnlp.engine.callbacks import CheckpointCallback, BestModelCallback\n",
    "from mindnlp.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imdb_train, imdb_test = load_dataset('imdb', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_dataset(dataset, tokenizer, max_seq_len=256, batch_size=32, shuffle=False):\n",
    "    def pad_sample(text):\n",
    "        if len(text) + 2 >= max_seq_len:\n",
    "            return np.concatenate(\n",
    "                [np.array([tokenizer.bos_token_id]), text[: max_seq_len-2], np.array([tokenizer.eos_token_id])]\n",
    "            )\n",
    "        else:\n",
    "            pad_len = max_seq_len - len(text) - 2\n",
    "            return np.concatenate( \n",
    "                [np.array([tokenizer.bos_token_id]), text,\n",
    "                 np.array([tokenizer.eos_token_id]),\n",
    "                 np.array([tokenizer.pad_token_id] * pad_len)]\n",
    "            )\n",
    "\n",
    "    column_names = [\"text\", \"label\"]\n",
    "    rename_columns = [\"input_ids\", \"label\"]\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    # map dataset\n",
    "    dataset = dataset.map(operations=[tokenizer, pad_sample], input_columns=\"text\")\n",
    "    # rename dataset\n",
    "    dataset = dataset.rename(input_columns=column_names, output_columns=rename_columns)\n",
    "    # batch dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "gpt_tokenizer = GPTTokenizer.from_pretrained('openai-gpt')\n",
    "\n",
    "# add sepcial token: <PAD>\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<bos>\",\n",
    "    \"eos_token\": \"<eos>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "}\n",
    "num_added_toks = gpt_tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train and valid datasets\n",
    "imdb_train, imdb_val = imdb_train.split([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = process_dataset(imdb_train, gpt_tokenizer, shuffle=True)\n",
    "dataset_val = process_dataset(imdb_val, gpt_tokenizer)\n",
    "dataset_test = process_dataset(imdb_test, gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(26283:140321715054400,MainProcess):2023-05-07-01:30:27.894.87 [mindspore/train/serialization.py:1088] For 'load_param_into_net', 1 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(26283:140321715054400,MainProcess):2023-05-07-01:30:27.907.88 [mindspore/train/serialization.py:1090] score.weight is not loaded.\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.models import GPTForSequenceClassification\n",
    "from mindnlp._legacy.amp import auto_mixed_precision\n",
    "\n",
    "# set bert config and define parameters for training\n",
    "model = GPTForSequenceClassification.from_pretrained('openai-gpt', num_labels=2)\n",
    "model.pad_token_id = gpt_tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(model.config.vocab_size + 3)\n",
    "model = auto_mixed_precision(model, 'O1')\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=2e-5)\n",
    "\n",
    "metric = Accuracy()\n",
    "\n",
    "# define callbacks to save checkpoints\n",
    "ckpoint_cb = CheckpointCallback(save_path='checkpoint', ckpt_name='gpt_imdb_finetune', epochs=1, keep_checkpoint_max=2)\n",
    "best_model_cb = BestModelCallback(save_path='checkpoint', ckpt_name='gpt_imdb_finetune_best', auto_load=True)\n",
    "\n",
    "trainer = Trainer(network=model, train_dataset=dataset_train,\n",
    "                  eval_dataset=dataset_val, metrics=metric,\n",
    "                  epochs=3, loss_fn=loss, optimizer=optimizer, callbacks=[ckpoint_cb, best_model_cb],\n",
    "                  jit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train will start from the checkpoint saved in 'checkpoint'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57888e72383746e3a634441c1e5d6973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 'gpt_imdb_finetune_epoch_0.ckpt' has been saved in epoch: 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e032e3fda7e645f2aa134e8f231e96cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.9081333333333333}\n",
      "---------------Best Model: 'best_so_far.ckpt' has been saved in epoch: 0.---------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fb78a0eaca4dc89e450b7bcba2553f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 'gpt_imdb_finetune_epoch_1.ckpt' has been saved in epoch: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d96ef52fc2c421284eac051b9ecd876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.9422666666666667}\n",
      "---------------Best Model: 'best_so_far.ckpt' has been saved in epoch: 1.---------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6304011679c148d7815afed463c63160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of stored checkpoints has been reached.\n",
      "Checkpoint: 'gpt_imdb_finetune_epoch_2.ckpt' has been saved in epoch: 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93760b8217784b9291a9f661c8261cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.9485333333333333}\n",
      "---------------Best Model: 'best_so_far.ckpt' has been saved in epoch: 2.---------------\n",
      "Loading best model from 'checkpoint' with '['Accuracy']': [0.9485333333333333]...\n",
      "---------------The model is already load the best model from 'best_so_far.ckpt'.---------------\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "trainer.run(tgt_columns=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a239acbf985c45d190195f0f9e592cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.90912}\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(network=model, eval_dataset=dataset_test, metrics=metric)\n",
    "evaluator.run(tgt_columns=\"label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
